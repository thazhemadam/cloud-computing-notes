# Lecture 2

## Parallel Computing

* Large problem is broken down into similar smaller independent subproblems that are executed by multiple processors simultaneousy. Results obtained are collated back to get the correct output 
* Primary Goal: Increase available computation power
* 4 Degrees of Parallelism:
  * Bit-level
  * Instruction-level
  * Task-level
  * Data-level

### 1. Bit Level Parallelism

* Based on increasing processor word size
* Ex: Consider adding 2 16-bit integers on a 8-bit CPU. Typicially, add 8 higher-order bits, then lower order bits, and then combine the A+B obtained. Instead, if this is added on a 16 bit processor, then this addition (A+B) can be done in one step.

### 2. Instruction Level Parallelism

* Simultaneous execution of multiple instructions
* Pipelineing is a form of ILP. Must be exploited to achieve parallel execution of instructions in instruction stream
* Ex:
  for(int i = 1; i <= 100; ++i)
    y[i] = y[i] + x[i]
  This is a parallel loop. Every iteration can be executed independently in parallel to any other iteration.

### 3. Data Level Parallelism

* Concurrent execution of same task on each multiple computing cores.
* Ex: Sum of all elements in an array. [0 ... N-1] for single core. [0...N/2 - 1] and [N/2 ... N-1] on two cores respectively, followed by [0...N/2 - 1] + [N/2 ... N-1].

### 4. Task Level Parallelism

* Concurrent execution of different task on multiple computing cores.
* Ex: Different cores performing unique statistical operations on data.

* Parallel Computing Architectures
* Two Types:
  * Multi-core computing
    * Single chip/processor. Multiple separate processing cores.
    * Communication via shared memory region generally.
  * Symmetric multiprocessing
    * Separate processors.
    * All separate processors connected to one another via a single shared main memory.
    * Single operating system

## Distributed Computing

* Distributed system components located on different networked computers
* Communication via RPC (remote procedure calls) like SOAP/REST API
* Computed results are used/collated as required

## Massively Parallel Computing

* Use numerous computers/processors to simultaneously execute in parallel
* 2 approaches:
  * Group several processors - tightly structured, centralized cluster.
  * Grid computing - Widely distributed computers work together and communicate via the internet. Predated version of distributed computing architecture

## Software perspective and solutions for Parallel Computing

### 1. Application Checkpointing

* Provides fault tolerance
* Records application's current variable states - enables it to restore and restart from that point in the instance of failure.
* Ex: Suppose an instruction being executed in parallel in the following way.
  Processor 1  O------x\---------------
          \
  Processor 2  O---------O----\--------
          \
  Processor 3  O----------------O------

  Suppose Processor 1 crashes at x. The computing work done by Processor 2 and Processor 3 until that point must not have been in vain, and they should not be made to wait either.
  So, Processor 1 must be able to recover to its state at x, and come to the point where it can pass on information to Processor 2.

### 2. Automatic Parallelization

* Conversion of sequential code into multithreaded code (for code instances such as `Ex. under Instruction Level Parallelism`)
Parallel programming languages
* Distributed Memory programming languages: Message passing to communicate
* Shared Memory programming languages: Manipulate of shared memory variables to communicate

### 3. Parallel Programming Languages

Language that you use must support parallelization and features for parallel programming.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Grid Computing

* Coordinated resource sharing and problem solving in dynamic, multi-institutional organizations
* Primary Goal: Delivering computing as a utility
* Was meant to be used by individual users who gain access to computing devices without knowing the details (geographical and tech specs) of the resource.
* Was limited to enabling shared resources for access with common, open, general purpose interfaces and protocols for access.
* No centralized control
* Was supposed to deliver non-trivial quality of service
* Emphasis was given to handling heterogeneous architectures (different resources and computers connected could be running on different architectures and OSes)
* Trust issues! - Establishing trust and security models was important (bc no centralized control)
* "Resource sharing agreements" - formed bw participating parties. Here, "sharing" = Direct Access to the resources
* Highly controlled sharing - resource providers and consumers were grouped into virtual organizations (based on sharing conditions)

## Virtual Organizations

* Dynamic collection of individuals/institutions from multiple administrative domains
* Basic unit for enabling access to shared resources (spec. resource-sharing policies applicable for users from a VO)
* Grid Tech allows for sharing of resources bw parties who may not have trust earlier
* Whatever access controls are established in a virtual organization, accordingly make resources accessible in either a unidirectional/bidirectional fashion between members of the virtual organization

### Benefits of Grid Computing

* Use underutilized resources
* Resource load balancing - tasks could be taken, split, processed, and collated
* Virtualize resources across enterprise
* Data grids and compute grids
  * Data grid: Deals with data and sgarubg if data - controlled sharing, managements of large amounts of distributed data.
  * Compute grid: Deals with providing resources for computation
        - "h/w and s/w infrastructure that provides dependable, consistent, pervasive, and inexpensive access to high-end computational capabilities"

### How does this operate? Typical view of Grid Environment

* There exist grid resources.
* 1. The details of the grid resource are collected by the Grid Information Service.
* 2. This information is passed from the Grid Information Service to the Resource Broker. Now the Resource Broker knows about the grid resources.
* 3. A user sends computation to resource broker via an application ("Grid Application").
* 4. Resource broker checks users' deets, and checks grid resources available as per user's credentials (and based on virtual organization rules set up) and their QoS requirements.
* 5. Accordingly, resources are assigned, computation is done (appropriate steps are taken otherwise), and then results are returned to the user.

### Layering grid Architecture

* Analogous to internet architecture

  * Application Layer User - *Application*
  Sends the request from User's end (the grid application)

  * Collective - *Application*
  Coordinates multiple resources -
    * ubiquitous infrastructure services
    * app-specific distributed services

  Collective has collective app, and collective generic
  * App -
    * Coherency control, replica selection, task management, virtual data, and data code catalog
  * Generic -
    * Replica catalog, replica management, co-allocation, certificate authorities, metadata

  * Resource - *Application*
  Sharing single resources -
    * Negotiating access
    * Controlling use
  
  * Connectivity - *Transport + Internet*
  Talking to things -
    * Communication (Internet Protocols)
    * Security

  * Fabric - *Link Layer*
  Controls things locally -
    * Access to and control of resources

* Example - Underage boy going to buy booze!
-> Boy (**Application**) goes to buy booze.
-> Assume shopkeeper is lazy. He doesn't wanna go till the fridge to check what all he has in stock, so he maintains a list (virtual catalogs maintained by **Collective**). First shopkeeper checks his catalog and all.
-> If it is there, then he walks over to the fridge, and maps the items the boy wanted to the items in the fridge (**Resource** - walking over to the fridge, mapping items in collective's virtual catalogs to the **Fabric**, i.e., booze bottles).
-> Then he remembers to check the boy's id (**Connect** - communication, authentication, authorization, delegation), and sells/doesn't sell (resource allocation) accordingly.

### Grid ware

* Used to (when it existed) manage the layered-grid architecture
* Special type of middleware
* Enable sharing and manage grid components - based on user requirements and resource attributes
* Today we don't have Grid Ware, thanks to web services, and APIs available
* Ex: Globus, Legion, IBP, NetSolve

## Differences! (finally)

So far, *Parallel Computing*, *Cloud Computing*, *Distributed Computing*, and *Grid Computing* have been introduced. Here are the differences between all these Paradigms.

### Parallel Computing vs Cloud Computing

* Parallel computing is an architecture. Cloud Computing enables this architecture.

* Cloud computing is a paradigm in software development which provides services (IAAS, PAAS, SAAS) that enable parallel computing, and/or  better storage options.

* Ex for the services provided:
  * IAAS:  2GB RAM, 50GB Disk
  * PAAS:  OS + DB
  * SAAS:  Payroll system that performs functions as and when they're called (Ex: f1("Enable Disk"); f2(Run CRM);)

* Parallel computing infrastructure is what allows you to use all the services provided by cloud computing. (For example above, you can say something like 'print("random text")' only using parallel computing ideas.)

### Parallel Computing vs Distributed Computing

* Refer to link: https://www.geeksforgeeks.org/difference-between-parallel-computing-and-distributed-computing/

### Distributed Computing vs Cloud Computing

* Refer to link: https://www.javatpoint.com/cloud-computing-vs-grid-computing

### Distributed Computing vs Grid Computing

* Grid Computing has an individual resource manager, and resources allocated dynamically at runtime.
* Distributed Computing has a centralized resource manager, and resources are not allocated dynamically.
