Lecture 2

Parallel Computing
-> Large problem is broken down into similar smaller independent subproblems that are executed by multiple processors simultaneousy. Results obtained are collated back to get the correct output 
-> Primary Goal: Increase available computation power
-> 4 Degrees of Parallelism:
	-> Bit-level
	-> Instruction-level
	-> Task-level
	-> Data-level

1. Bit Level Parallelism
-> Based on increasing processor word size
-> Ex: Consider adding 2 16-bit integers on a 8-bit CPU. Typicially, add 8 higher-order bits, then lower order bits, and then combine the A+B obtained. Instead, if this is added on a 16 bit processor, then this addition (A+B) can be done in one step.

2. Instruction Level Parallelism
-> Simultaneous execution of multiple instructions
-> Pipelineing is a form of ILP. Must be exploited to achieve parallel execution of instructions in instruction stream
-> Ex:
	for(int i = 1; i <= 100; ++i)
		y[i] = y[i] + x[i]
	This is a parallel loop. Every iteration can be executed independently in parallel to any other iteration.

3. Data Level Parallelism
-> Concurrent execution of same task on each multiple computing cores.
-> Ex: Sum of all elements in an array. [0 ... N-1] for single core. [0...N/2 - 1] and [N/2 ... N-1] on two cores respectively, followed by [0...N/2 - 1] + [N/2 ... N-1].

4. Task Level Parallelism
-> Concurrent execution of different task on multiple computing cores.
-> Ex: Different cores performing unique statistical operations on data.

-> Parallel Computing Architectures
-> Two Types:
	-> Multi-core computing
		-> Single chip/processor. Multiple separate processing cores.
		-> Communication via shared memory region generally.
	-> Symmetric multiprocessing
		-> Separate processors.
		-> All separate processors connected to one another via a single shared main memory.
		-> Single operating system

Distributed Computing
-> Distributed system components located on different networked computers
-> Communication via RPC (remote procedure calls) like SOAP/REST API
-> Computed results are used/collated as required

Massively Parallel Computing
-> Use numerous computers/processors to simultaneously execute in parallel
-> 2 approaches:
	-> Group several processors - tightly structured, centralized cluster.
	-> Grid computing - Widely distributed computers work together and communicate via the internet. Predated version of distributed computing architecture

Software perspective and solutions for Parallel Computing
-> Application Checkpointing
	-> Provides fault tolerance
	-> Records application's current variable states - enables it to restore and restart from that point in the instance of failure.
	-> Ex: Suppose an instruction being executed in parallel in the following way.
	Processor 1	O------x\---------------
			 	 \
	Processor 2	O---------O----\--------
					\
	Processor 3	O----------------O------

	Suppose Processor 1 crashes at x. The computing work done by Processor 2 and Processor 3 until that point must not have been in vain, and they should not be made to wait either.
	So, Processor 1 must be able to recover to its state at x, and come to the point where it can pass on information to Processor 2.
-> Automatic Parallelization
	-> Conversion of sequential code into multithreaded code (for code instances such as `Ex. under Instruction Level Parallelism`)
Parallel programming languages
	-> Distributed Memory programming languages: Message passing to communicate
	-> Shared Memory programming languages: Manipulate of shared memory variables to communicate

Difference between parallel computing and cloud computing
-> Cloud computing is a paradigm in software development which provides services (IAAS, PAAS, SAAS) that enable parallel computing, and/or  better storage options.
-> Ex for the services provided:
	-> IAAS:	2GB RAM, 50GB Disk 
	-> PAAS:	OS + DB
	-> SAAS:	Payroll system that performs functions as and when they're called (Ex: f1("Enable Disk"); f2(Run CRM);)

Difference between parallel computing and distributed computing
-> Refer to link: https://www.geeksforgeeks.org/difference-between-parallel-computing-and-distributed-computing/

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Grid Computing
-> Coordinated resource sharing and problem solving in dynamic, multi-institutional organizations
-> Primary Goal: Delivering computing as a utility
-> Was meant to be used by individual users who gain access to computing devices without knowing the details (geographical and tech specs) of the resource.
-> Was limited to enabling shared resources for access with common, open, general purpose interfaces and protocols for access.
-> No centralized control
-> Was supposed to deliver non-trivial quality of service
-> Emphasis was given to handling heterogeneous architectures (different resources and computers connected could be running on different architectures and OSes)
-> Establishing trust and security models was important (bc no centralized control)
-> "Resource sharing agreements" - formed bw participating parties. Here, "sharing" = Direct Access to the resources
-> Highly controlled sharing - resource providers and consumers were grouped into virtual organizations (based on sharing conditions)

Virtual Organizations
-> Dymamic collection of individuals/institutions from multiple administrative domains
-> Basic unit for enabling access to shared resources (spec. resource-sharing policies applicable for users from a VO)
-> Grid Tech allows for sharing of resources bw parties who may not have trust earlier
Benefits of VO
-> Use underutilized resources
-> Resource load balancing - tasks could be taken, split, processed, and collated
-> Virtualize resources across enterprise
-> Data grids and compute grids
	-> Data grid: Deals with data - controlled sharing, managements of large amounts of distributed data.
	-> Compute grid: Deals with providing resources for computation 
			  - "h/w and s/w infrastructure that provides dependable, consistent, pervasive, and inexpensive access to high-end computational capabilities"

Typical view of Grid Environment : Refer to ppt. for diagram


Layering grid Architecture - Analogy to internet architecture : Refer to ppt. for diagram

Data grid architecture : Refer to ppt. for diagram (focus on the phrases in blue as definitions)

Grid ware
-> Special type of middleware
-> Enable sharing and manage grid components - based on user requirements and resource attributes
-> Ex: Globus, Legion, IBP, NetSolve

Difference between grid computing and distributed computing
-> Refer to link: https://www.javatpoint.com/cloud-computing-vs-grid-computing

