# Lecture 2

## Trouble with Distributed Systems

Many things can go wrong.

* Request lost
* Request waiting in a queue
* Failed remote node
* Non-responsive remote node
* Request processed, response lost
* Request processed, response delayed

```text
        (a)              (b)             (c)                    time
Client  --------- - - -  --------- - - - - - ------------------ - - - ->
            \               \                   \
Network - - -X- - - - - - - -\- - - - - - - - - -\- - --X
                              X                   \    / 
Service  - - - - - - -||node unresponsive||- - - - ---- - - - - - - - - -
```

### Partial Failure

* Some parts of the systems are broken, even though other parts are working fine.
* Non-deterministic in nature

## Reliability, Faults, and Failures

### What is Reliability?

Let's define "working correctly" as -

* Deterministic - principle of least astonishment for users
* can tolerate user being an idiot (or really smart) and messing around
* delivers decent performance under expected load and data volume
* secure - prevents unauthorized access, abuse

Then, Reliability is "continuing to work correctly, even when things go wrong".

* "Things that can go wrong" - faults
* Systems that can anticipate faults and cope with them are called "fault-tolerant"/"resilient"
  
* Reliability - probability that the system will meet certain performance standards in yielding correct output for a desired time duration.
* Metric to measure reliability - MTBF - Mean Time Between Failures - average operating time between one failure and the next

  ```text
  MTBF =  Total Uptime
        _________________
         # of breakdowns
  ```

### What is "Failure"? What exactly is a "Fault"?

* A system is said to "fail" when it cannot meet its requirements.
* Failure is brought about by the existence of errors in the system.
* Cause of the errors are called faults.

### TLDR; Fault vs Failure

* Fault - one component of the system deviating from its specification
* Failure - whole system stops providing the required service to the user
  
### Types of Faults

* Transient     - appears once, and then no more
* Intermittent  - occurs, vanishes, reoccurs - without any real pattern
* Permanent     - once it occurs, you gotta replace/repair the faulty component to fix things

### Types of Failures

* Crash Failure         -   server was working correctly. but halts.
* Omission Failure      -   server fails to respond to requests
  * Receive omission    -   server fails to receive incoming messages
  * Send omission       -   server fails to send messages
* Timing Failure        -   server's response lies outside specified time interval
* Response Failure      -   server's response is incorrect (due to data inconsistency usually)
  * Value failure       -   server's response value is wrong
  * State transition failure    - server deviates from correct flow of control
* Arbitrary failure     -   random. server produces arbitrary failures at arbitrary times.

## Failure Models

* Crash-stop faults - system crashes. there is no chance of recovering the node.
* Crash-recovery faults - nodes may crash at any moment, BUT may start responding again after some unknown time, and can be recovered. Persistent storage (assumed to be present) is preserved, but in-memory state is lost
* Fail-arbitrary/Byzantine faults - random behaviour. random type of failure. full chaos. server produces output it shouldn't have ever produced, but which can't be detected as being incorrect. fault server could also be working with other servers to intentionally produce wrong answers. pretends to work correctly, when they're faulty. (most dangerous)

## Fault Detection

* Timeouts and Unbounded Delay -
  * Specify a timeout 'd'. All packets sent are delivered within time `d`, or is lost. Delivery never takes longer than `d`.
  * For some guaranteed time `r` defined (which is required for a non-failed node to process request), response should arrive no later than `2d + r`. Response never takes longer than `2d + r`.
  * Unfortunately, most systems can't guarantee either of these - asynchronous networks have unbounded delays.\
    -> Try to deliver packets as quickly as possible. But there's no upper limit on time it may take for a packet to arrive (and most server implementations can't guarantee a bound either).

* Network Congestion and Queueing
  * Essentially the same concept as that in Computer Networks
  * Variability of packet delay is mostly because of queueing
  * If switch queue fills up, packet is dropped and needs to be resent (even if network is ok)
  * In virtualized environments, running OS  is paused for `ms` when another VM uses a CPU core\
  During this time, another VM can't consume any data, so the VMM queues the incoming data
  * Also, TCP considers a packet to be lost if it isn't ACK-ed within some timeout
  * App doesn't see the packet-loss/retransmission, it does feel the resulting delay

* In public clouds, multi-tenant data centers, resources are shared among many customers. One user hogging all the resources can cause delays for other users (batch workloads like MapReduce can saturate network links)

* How to choose timeouts then? Do it *Experimentally*.
  * Measure distribution of network RTTs over a long period, over many machines, and determine degree of variability
  * Consider the app's characteristics and decide on an apt trade-off between failure detection delay and risk of premature timeouts
  * Systems can continually measure response times, and their variability (jitter) and auto-adjust time-outs according to observed time distribution.
  * Ex - Phi Accrual failure detector

* Synchronous v/s Asynchronous Networks
  * Synchronous Circuit Switching doesn't suffer from queueing - max end to end latency is fixed - bounded delay
  * But, having synchronous networks isn't feasible (dedicated networks, etc)
  * Data centers use packet switching (as they're optimized for bursty traffic) - but suffer from queueing & unbounded delay
  * If resources are statically partitioned, latency delays can be guaranteed (but this is more expensive, as it comes at cost of reduced utilization). Dynamic partitioning - cheaper, but variable delays (a cost/benefit trade-off)

* Currently deployed tech doesn't allow us to make such any guarantees (on delays, network reliability), so assume the worst while designing your system. No "correct" value for timeouts either - must be experimentally determined

## Failure Detection in Distributed System

Heartbeat!

* All-to-All heartbeat - every node sends out its heartbeats to every other node - more noise and causes a lot of traffic
* Centralized Heartbeat - all nodes send out their heartbeats to a centralized node - single point of failure
* Ring Heartbeat - nodes send out their heartbeats to the immediate next node in a ring - multiple points of failure

## Fault Tolerance in Distributed Systems

Recovering from faults is tough.

* No global variables, shared memory (memory isn't always shared between systems), or common knowledge, or any kind of shared state between the machines.
* Nodes can't even agree on time, let alone anything else

So, we can't depend on a single node to achieve fault detection, or fault tolerance.

Bottom line is, we need protocols that take several nodes into consideration for fault tolerance, i.e., use quorums.

### Byzantine Fault Tolerance

* Systems that can tolerate Byzantine Fault Tolerance, i.e., continue to operate correctly, even if some nodes are malfunctioning, and disobeying protocol, or malicious attackers are interfering with the network
* Byzantine agreement - All nodes share info about themselves, as well as others
* This allows us to identify faulty nodes, and detect faults
* If there's a majority that behaves consistently, then a consensus can be reached

## Failover Architecture

Failover - switching to a redundant or standby service (server, system, hardware device, or network), when another service fails/abnormally terminates.

* Active-Active Architecture
  * Symmetric - all nodes are active
  * Each server is configured to run a specific app, and provide redundancy for its peer.
  * Each node is capable on taking on the additional workload. Failed over services (and traffic intended for failed node) are passed onto an existing node or load balanced over other nodes.
  * Database instances run concurrently on both servers, and access the same data on the shared storage. These instances must communicate with one another to manage access to shared data in database.
  * In case of a server failure, the remaining server can continue processing the workload, failed sessions can be reconnected.

* Active-Passive/Active-Standby Architecture
  * Asymmetric - one node is active, the other is standby
  * Applications run on a master/primary server
  * Dedicated redundant server is present to take over if master fails
  * Database applications (single instances) are installed on both nodes. But, database is located on shared storage.
  * Normally, database instance runs only on active node. If it fails - clustering software - transfer control of disk subsystem to secondary system/server. Database instance on second node is started, and service is resumed.

* There is a performance requirement-resource utilization tradeoff between both the approaches.
