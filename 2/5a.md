# Lecture 5a

## Memory Virtualization

## Some useful page-related information

* Every process has a page table created, which lies in the user space
* CR3 register is loaded with base address of relevant page table when a page is to be fetched/modified. Then, a page walker walks through the page table till it tries to find the required virtual page number.

## Memory Virtualization Requirements

* No direct access to physical memory from VM
* Only hypervisor should manage physical memory
* But, guest OS should think it is accessing physical memory

There are few ways by which memory virtualization can be done.

* Using Nested Page Table
* Using a Shadow Page Table
* Using an Extended Page Table (EPT)

## Virtualizing Virtual Memory

* Page tables are in some form of a tree structure (they're nested, and it's easier to walk than a linear structure)

```text

                    VM - 1
            ______________________
Guest       |    App1       App2 |
Page       -|    \         /     |
table       |   __\_______/__    |
            |   |_|_|_|_|_|_|    |
            |-------\----/-------|  
             Managed by Hypervisor
              _______\___/_______
              |_|_|_|_|_|_|_|_|_|   } -> Real Physical Memory
```

* Generalized - In an OS, a process P1 makes an attempt to access page table, by giving a pointer to the Virtual Page address. The walker will walk down the tree, and find the relevant VP address, find the physical address and return that for further processing

* Physical page management in a virtualized environment must be done by the Hypervisor. Guest OS is in a VM. Guest OS doesn't have access to the actual physical memory. Hypervisor does. So, when a process V_P1 tries to get the physical page for a virtual page, it makes no sense, because the page table in the guest OS doesn't know the true physical page. So, what do we do?

## Shadow Page Table

* Shadow paging was a software only solution (before HW support was implemented)

* Idea - Let hypervisor create shadow page table that maps guest VA to host PA directly
  * Shadow table is made by combining Guest Page Table with System Page Table
  * Shadow Page tables map guest-virtual pages directly to host-physical pages
  * Hypervisor makes CR3 point to the root of the shadow page table
  * Page Table Walker walks the shadow page table

* Create a second copy of page tables called shadow page tables that map virtual addresses directly to host physical addresses, and let the processor use them for address translation (so, CR3 points to shadow page table, not primary/guest page tables)
* Structure of Shadow Page Table must be same as the Primary Page Tables kept by the Guest OS.
* Naive approach to keep these in sync -
  * Let Guest OS freely modify page tables when page fault occurs.
  * Hypervisor intercepts only INVLPG (invalidate TLB entry) privileged instruction
  * Then, hypervisor makes corresponding changes to its shadow page tables, and substitute Guest Physical Address with corresponding Host Physical Address
  * This doesn't always work, because some OS forget to invalidate TLB entries
* Fallback approach -
  * Write-protect all Guest Page Tables
  * Whenever OS tries to modify these, a page fault occurs, which is directed/vectored into the hypervisor
  * Hypervisor immediately updates Shadow Page Table
  * Both structures (Guest Page Table and Shadow Page Table) mirror each other (modulo physical addresses)
  * This costs a large number of page faults (one page fault for each access). Fault handling routines are expensive
* Common optimization -
  * Allow shadow tables to be lazily updated (possible only if not all primary page tables are write protected)
  * If entry is present from primary page table, but absent in shadow page table, then page fault will occur when walker walks it for first time.
  * This is serviced by hypervisor, which creates an entry in the Shadow Page Table based on what's in the Guest Page Table

## Challenges with Shadow Page Tables

* There is one Shadow Page Table for every guest process
* Every time Guest Application does context switching, a trap is vectored into the hypervisor to change CR3 to point to the corresponding shadow table
* Maintaining consistency between Guest Page Tables and Shadow Page Tables can lead to a lot of overhead (VMM Traps)
* Loss of Performance due to TLB flushing on every "world-switch" (switching between VMs)
* Memory overhead due to Shadow copying Guest Page Tables

## Nested/Extended Page Tables

* Hardware support for memory virtualization
* Set up by hypervisor to perform a second level of address translation

* Idea - Make hardware aware of two levels of address translation, and of Guest and Nested Page Table
  * Hardware Page Walker will walk on both Guest Page Table and Nested Page Table (Two Dimensional Page walk) on a TLB miss
  * There are 2 CR3s - gCR3 for guest Page Table, nCR3 for nested Page Table
  * Eliminates need for a shadow page table

## Nested Page Tables

* Hypervisor intervenes only when page faults occur
* Two Stage Process
  * 1. OS Processes the fault. Guest OS allocates guest physical memory from its fake pool and stores it in Guest Page Table
  * 2. User instruction that caused the fault is restarted. Address translation progresses half way and faults when it can't find actual guest-host mapping (entry isn't present in the Nested Page Table). Fault is vectored into the Hypervisor, which allocates physical memory, and updates the rest of the page tables.

## Extended Page Tables

* A new page table structure, under control of VMM
  * Mapping between Guest Physical Address and Host Physical Address
  * EPT base pointer (VMCS field) points to EPT page Tables
  * EPT activated on VM entry, deactivated on VM Exit
* Guest OS has full control over its own IA-32 page tables
  * No VM exits due to guest page faults, INVLPG, or CR3 changes

## EPT - Performance, and Benefits

* EPT benefit is very dependent on workload
  * Typically expected to be up to 20%
  * Benefit increases with number of vCPUs
* No need for complex page table visualization and maintenance algorithms
* Reduced memory footprint as compared to shadow-page table algorithms
  * Single EPT supports entire VM, while each Guest process needed a Shadow Page Table
